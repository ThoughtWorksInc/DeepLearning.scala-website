{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "In this article, we will use [softmax](https://en.wikipedia.org/wiki/Softmax_function) classifier to build a simple image classification neural network with an accuracy of 32%. In a Softmax classifier, binary logic is generalized and regressed to multiple logic. Softmax classifier will output the probability of the corresponding category.\n",
    "\n",
    "We will first define a softmax classifier, then use the training set of [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) to train the neural network, and finally use the test set to verify the accuracy of the neural network.\n",
    "\n",
    "Letâ€™s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous course [GettingStarted](https://thoughtworksinc.github.io/DeepLearning.scala/demo/GettingStarted.html), we need to introduce each class of DeepLearning.scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                            \n",
       "\n",
       "// import scala.concurrent.ExecutionContext.Implicits.global\n",
       "\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DeepLearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.plugins._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.feature.Factory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.future._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.concurrent.Await\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.concurrent.duration.Duration\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.each.Monadic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscalaz.std.stream._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// import $ivy.`org.nd4j::nd4s:0.9.1`\n",
    "// import $ivy.`org.nd4j:nd4j-cuda-8.0-platform:0.9.1`\n",
    "import $ivy.`org.nd4j::nd4s:0.8.0`\n",
    "import $ivy.`org.nd4j:nd4j-native-platform:0.8.0`\n",
    "import $ivy.`com.chuusai::shapeless:2.3.2`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::plugins-builtins:2.0.1`\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.2`\n",
    "import $ivy.`com.thoughtworks.each::each:3.3.1`\n",
    "\n",
    "import $ivy.`com.thoughtworks.each:each_2.11:3.3.1`\n",
    "\n",
    "import $plugin.$ivy.`org.scalamacros:paradise_2.11.11:2.1.0`\n",
    "\n",
    "// import scala.concurrent.ExecutionContext.Implicits.global\n",
    "\n",
    "\n",
    "\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import com.thoughtworks.deeplearning.DeepLearning\n",
    "import com.thoughtworks.deeplearning.plugins._\n",
    "import com.thoughtworks.feature.Factory\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "plotly.JupyterScala.init()\n",
    "\n",
    "import com.thoughtworks.future._\n",
    "import scala.concurrent.Await\n",
    "import scala.concurrent.duration.Duration\n",
    "import com.thoughtworks.each.Monadic._\n",
    "import scalaz.std.stream._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the line numbers outputted by `jupyter-scala` and to make sure that the page output will not be too long, we need to set `pprintConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprintConfig() = pprintConfig().copy(height = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your own neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate need to be set for the full connection layer. Learning rate visually describes the change rate of `weight`. A too-low learning rate will result in slow decrease of `loss`, which will require longer time for training; A too-high learning rate will result in rapid decrease of `loss` at first while fluctuation around the lowest point afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mINDArrayLearningRatePluginUrl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"https://gist.githubusercontent.com/issimo-sakura/f06279e648e45bd574dc382abb4c44ac/raw/7bd7a871030988c58524108c5985f71002f82012/INDArrayLearningRate.sc\"\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val INDArrayLearningRatePluginUrl = \"https://gist.githubusercontent.com/issimo-sakura/f06279e648e45bd574dc382abb4c44ac/raw/7bd7a871030988c58524108c5985f71002f82012/INDArrayLearningRate.sc\"\n",
    "interp.load(scala.io.Source.fromURL(new java.net.URL(INDArrayLearningRatePluginUrl)).mkString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mCNNsPluginUrl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"https://gist.github.com/Atry/15b7d9a4c63d95ad3d67e94bf20b4f69/raw/59f7ee4dff0dde3753f560633574265e950edc93/CNN.sc\"\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val CNNsPluginUrl = \"https://gist.github.com/Atry/15b7d9a4c63d95ad3d67e94bf20b4f69/raw/59f7ee4dff0dde3753f560633574265e950edc93/CNN.sc\"\n",
    "interp.load(scala.io.Source.fromURL(new java.net.URL(CNNsPluginUrl)).mkString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mL2RegularizationPluginUrl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"https://gist.githubusercontent.com/TerrorJack/a60ff752270c40a6485ee787837390aa/raw/119cbacb29dc12d74ae676b4b02687a8f38b02e4/L2Regularization.sc\"\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val L2RegularizationPluginUrl = \"https://gist.githubusercontent.com/TerrorJack/a60ff752270c40a6485ee787837390aa/raw/119cbacb29dc12d74ae676b4b02687a8f38b02e4/L2Regularization.sc\"\n",
    "interp.load(scala.io.Source.fromURL(new java.net.URL(L2RegularizationPluginUrl)).mkString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// val AdamPluginUrl = \"https://gist.githubusercontent.com/issimo-sakura/0c2fc6ba4cfa536e4788112a94200b50/raw/9f68360023ba7db17e7437a9501739ffaf375ce2/Adam.sc\"\n",
    "// interp.load(scala.io.Source.fromURL(new java.net.URL(AdamPluginUrl)).mkString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.concurrent.Executors\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.concurrent.ExecutionContext\n",
       "\u001b[39m\n",
       "\u001b[36msingleThreadExecutor\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mutil\u001b[39m.\u001b[32mconcurrent\u001b[39m.\u001b[32mExecutorService\u001b[39m = java.util.concurrent.Executors$FinalizableDelegatedExecutorService@627d04ca\n",
       "\u001b[36msingleThreadExecutionContext\u001b[39m: \u001b[32mconcurrent\u001b[39m.\u001b[32mExecutionContextExecutor\u001b[39m = scala.concurrent.impl.ExecutionContextImpl@5c1c5f1b"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.concurrent.Executors\n",
    "import scala.concurrent.ExecutionContext\n",
    "val singleThreadExecutor = Executors.newSingleThreadExecutor()\n",
    "implicit val singleThreadExecutionContext = ExecutionContext.fromExecutor(singleThreadExecutor, { throwable =>\n",
    "    println(\"xxxxx\")\n",
    "    sys.exit(-1)\n",
    "    throwable.getCause.printStackTrace()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// `interp.load` is a workaround for https://github.com/lihaoyi/Ammonite/issues/649 and https://github.com/scala/bug/issues/10390\n",
    "\n",
    "// with Adam\n",
    "interp.load(\"\"\"\n",
    "  val hyperparameters = Factory[Builtins with CNNs with INDArrayLearningRate  with L2Regularization ].\n",
    "  newInstance(learningRate = 0.0001, l2Regularization = 0.5)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `softmax` classifier (softmax classifier is a neural network combined by `softmax` and a full connection), we first need to write softmax function, formula: ![](https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhyperparameters.implicits._\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hyperparameters.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhyperparameters.INDArrayLayer\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hyperparameters.INDArrayLayer\n",
    "\n",
    "def softmax(scores: INDArrayLayer): INDArrayLayer = {\n",
    "  val expScores = hyperparameters.exp(scores)\n",
    "  (expScores + 1e-8) / (expScores.sum(1) + 1e-8)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfileHandler\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mutil\u001b[39m.\u001b[32mlogging\u001b[39m.\u001b[32mFileHandler\u001b[39m = java.util.logging.FileHandler@41ae7095"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileHandler = new java.util.logging.FileHandler(\"CNNsmall%g.log\")\n",
    "hyperparameters.logger.addHandler(fileHandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose your  neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a full connection layer and [initialize Weight](https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started#231--weight-intialization), `Weight` shall be a two-dimension `INDArray` of `NumberOfPixels Ã— NumberOfClasses`. `scores` is the score of each image corresponding to each category, representing the feasible probability of each category corresponding to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.etl.Cifar10\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.future._\n",
       "\u001b[39m\n",
       "\u001b[36mcifar10\u001b[39m: \u001b[32mcom\u001b[39m.\u001b[32mthoughtworks\u001b[39m.\u001b[32mdeeplearning\u001b[39m.\u001b[32metl\u001b[39m.\u001b[32mCifar10\u001b[39m = \u001b[33mCifar10\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.thoughtworks.deeplearning.etl::cifar10:1.1.0`\n",
    "import com.thoughtworks.deeplearning.etl.Cifar10\n",
    "import com.thoughtworks.future._\n",
    "val cifar10 = Cifar10.load().blockingAwait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mNumberOfPixels\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3072\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//10 label of CIFAR10 images(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "val NumberOfPixels: Int = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhyperparameters.INDArrayWeight\n",
       "// case class CnnLayer(numberOfFilters: Int, hasPooling: Boolean)\n",
       "\n",
       "// // val cnnLayers: Array[CnnLayer] = Array(\n",
       "// //     CnnLayer(16, hasPooling = true),\n",
       "// //     CnnLayer(18, hasPooling = true),\n",
       "// //     CnnLayer(20, hasPooling = false),\n",
       "// //     CnnLayer(22, hasPooling = true),\n",
       "// //     CnnLayer(24, hasPooling = true)\n",
       "// // )\n",
       "\n",
       "// val cnnLayers: Array[CnnLayer] = Array(\n",
       "//     CnnLayer(64, hasPooling = false),\n",
       "//     CnnLayer(64, hasPooling = true),\n",
       "//     CnnLayer(128, hasPooling = false),\n",
       "//     CnnLayer(128, hasPooling = true),\n",
       "//     CnnLayer(256, hasPooling = false),\n",
       "//     CnnLayer(256, hasPooling = true)\n",
       "// )\n",
       "\n",
       "// val lastCnnWidth = cnnLayers.foldLeft(defaultPixelSize) { (width, cnnLayerConfigure) =>\n",
       "//     if (cnnLayerConfigure.hasPooling) {\n",
       "//     assert(width > 1)\n",
       "//         width / 2\n",
       "//     } else {\n",
       "//         width\n",
       "//     }\n",
       "// }\n",
       "// val outputPixel = cnnLayers.last.numberOfFilters * lastCnnWidth * lastCnnWidth\n",
       "\n",
       "// final case class Model(cnnLayerParameters: Seq[CnnLayerParameter],\n",
       "//                        fullConnectedWeight: INDArrayWeight,\n",
       "//                        fullConnectedBias: INDArrayWeight) \n",
       "\n",
       "// final case class CnnLayerParameter(weight: INDArrayWeight, bias: INDArrayWeight)\n",
       "\n",
       "// def isExistVersion(version: Int): Boolean = {\n",
       "//     import ammonite.ops._\n",
       "//     val filePath = pwd / \"backup\" / version.toString\n",
       "//     exists(filePath)\n",
       "// }\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hyperparameters.INDArrayWeight\n",
    "// case class CnnLayer(numberOfFilters: Int, hasPooling: Boolean)\n",
    "\n",
    "// // val cnnLayers: Array[CnnLayer] = Array(\n",
    "// //     CnnLayer(16, hasPooling = true),\n",
    "// //     CnnLayer(18, hasPooling = true),\n",
    "// //     CnnLayer(20, hasPooling = false),\n",
    "// //     CnnLayer(22, hasPooling = true),\n",
    "// //     CnnLayer(24, hasPooling = true)\n",
    "// // )\n",
    "\n",
    "// val cnnLayers: Array[CnnLayer] = Array(\n",
    "//     CnnLayer(64, hasPooling = false),\n",
    "//     CnnLayer(64, hasPooling = true),\n",
    "//     CnnLayer(128, hasPooling = false),\n",
    "//     CnnLayer(128, hasPooling = true),\n",
    "//     CnnLayer(256, hasPooling = false),\n",
    "//     CnnLayer(256, hasPooling = true)\n",
    "// )\n",
    "\n",
    "// val lastCnnWidth = cnnLayers.foldLeft(defaultPixelSize) { (width, cnnLayerConfigure) =>\n",
    "//     if (cnnLayerConfigure.hasPooling) {\n",
    "//     assert(width > 1)\n",
    "//         width / 2\n",
    "//     } else {\n",
    "//         width\n",
    "//     }\n",
    "// }\n",
    "// val outputPixel = cnnLayers.last.numberOfFilters * lastCnnWidth * lastCnnWidth\n",
    "\n",
    "// final case class Model(cnnLayerParameters: Seq[CnnLayerParameter],\n",
    "//                        fullConnectedWeight: INDArrayWeight,\n",
    "//                        fullConnectedBias: INDArrayWeight) \n",
    "\n",
    "// final case class CnnLayerParameter(weight: INDArrayWeight, bias: INDArrayWeight)\n",
    "\n",
    "// def isExistVersion(version: Int): Boolean = {\n",
    "//     import ammonite.ops._\n",
    "//     val filePath = pwd / \"backup\" / version.toString\n",
    "//     exists(filePath)\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// def writeWeightsAndBias(version: Int, model: Model): Unit = {\n",
    "//     import ammonite.ops._\n",
    "    \n",
    "//     def write(outputFilePath: Path, dataOfWeightOrBias: INDArrayWeight): Unit = {\n",
    "//         import java.io.FileOutputStream\n",
    "//         import java.io.ObjectOutputStream\n",
    "//         val outputStream: ObjectOutputStream = new ObjectOutputStream(new FileOutputStream(outputFilePath.toIO))\n",
    "//         try {\n",
    "//             outputStream.writeObject(dataOfWeightOrBias.data)\n",
    "//         } finally {\n",
    "//             outputStream.close()\n",
    "//         }\n",
    "//     }\n",
    "    \n",
    "//     val Model(cnnLayerParameters, \n",
    "//           fullConnectedWeight,\n",
    "//           fullConnectedBias) = model\n",
    "    \n",
    "        \n",
    "//     for (CnnLayerParameter(weight, bias) <- cnnLayerParameters; index <- cnnLayerParameters.indices) {\n",
    "//         //val CnnLayerParameter(weight, bias) = weightAndBias\n",
    "        \n",
    "//         val filePath = pwd / \"backup\" / version.toString\n",
    "//         val weightFilePath = filePath / s\"weight$index\"\n",
    "//         val biasFilePath = filePath / s\"bais$index\"\n",
    "//         write(weightFilePath, weight)\n",
    "//         write(biasFilePath, bias)\n",
    "\n",
    "//     }\n",
    "    \n",
    "//     val fullConnectedIndex = cnnLayers.length\n",
    "    \n",
    "//     val fullConnectedfilePath = pwd / \"backup\" / version.toString\n",
    "//     val fullConnectedweightFilePath = fullConnectedfilePath / s\"weight$fullConnectedIndex\"\n",
    "//     val fullConnectedbiasFilePath = fullConnectedfilePath / s\"bais$fullConnectedIndex\"\n",
    "    \n",
    "//     write(fullConnectedweightFilePath, fullConnectedWeight)\n",
    "//     write(fullConnectedbiasFilePath, fullConnectedBias)\n",
    "// }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// def readWeightsAndBias(version: Int): Model = {\n",
    "//     if (isExistVersion(version) == false) {\n",
    "//         throw new IllegalArgumentException(s\"The version$version isn't exist!\")\n",
    "//     }\n",
    "    \n",
    "//     import ammonite.ops._\n",
    "    \n",
    "//     def read(inputFilePath: Path): INDArrayWeight = {\n",
    "//         import java.io.FileInputStream\n",
    "//         import java.io.ObjectInputStream\n",
    "//         val inputStream: ObjectInputStream = new ObjectInputStream(new FileInputStream(inputFilePath.toIO))\n",
    "//         try {\n",
    "//             INDArrayWeight(inputStream.readObject().asInstanceOf[INDArray])\n",
    "//         } finally {\n",
    "//             inputStream.close()\n",
    "//         }\n",
    "//     }\n",
    "    \n",
    "//     val cnnLayerParameter = for (index <- cnnLayers.indices) yield {\n",
    "//         val filePath = pwd / \"backup\" / version.toString\n",
    "//         val weightFilePath = filePath / s\"weight$index\"\n",
    "//         val biasFilePath = filePath / s\"bais$index\"\n",
    "//         val weight = read(weightFilePath)\n",
    "//         val bias = read(biasFilePath)\n",
    "//         CnnLayerParameter(weight, bias)\n",
    "//     }\n",
    "//     val fullConnectedIndex = cnnLayers.length\n",
    "    \n",
    "//     val fullConnectedfilePath = pwd / \"backup\" / version.toString\n",
    "//     val fullConnectedweightFilePath = fullConnectedfilePath / s\"weight$fullConnectedIndex\"\n",
    "//     val fullConnectedbiasFilePath = fullConnectedfilePath / s\"bais$fullConnectedIndex\"\n",
    "    \n",
    "//     val fullConnectedWeight = read(fullConnectedweightFilePath)\n",
    "//     val fullConnectedBias = read(fullConnectedbiasFilePath)\n",
    "    \n",
    "//     Model(cnnLayerParameter, fullConnectedWeight, fullConnectedBias)\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// def initializeWeightAndBias(version: Int): Model = {\n",
    "//     import org.nd4s.Implicits._\n",
    "//     def NumberOfChannels = 3 // magic Number\n",
    "//     def loadWeightAndBias() = {\n",
    "//         readWeightsAndBias(version)\n",
    "//     }\n",
    "\n",
    "//     def randomlyInitializeWeightAndBias() = {\n",
    "        \n",
    "//         val cnnLayerParameter = for (i <- cnnLayers.indices) yield {\n",
    "//             val inputDepth = if (i == 0) {\n",
    "//                 NumberOfChannels\n",
    "//             } else {\n",
    "//                 cnnLayers(i - 1).numberOfFilters\n",
    "//             }\n",
    "            \n",
    "\n",
    "//             val numberOfFilters = cnnLayers(i).numberOfFilters\n",
    "//             val weight = INDArrayWeight(\n",
    "//                 Nd4j.randn(\n",
    "//                     Array(numberOfFilters, \n",
    "//                     inputDepth, \n",
    "//                     KernelWidth, \n",
    "//                     KernelHeight)\n",
    "//                 ) / math.sqrt(inputDepth * KernelWidth * KernelHeight / 2))\n",
    "            \n",
    "//             val bias = INDArrayWeight(Nd4j.zeros(numberOfFilters))\n",
    "            \n",
    "//             CnnLayerParameter(weight, bias)\n",
    "//         }\n",
    "        \n",
    "//         val numberOfFilters = cnnLayers.last.numberOfFilters\n",
    "//         val fullConnectedWeight = INDArrayWeight(Nd4j.randn(Array(outputPixel, Cifar10.NumberOfClasses)) / math.sqrt(outputPixel / 2))\n",
    "//         val fullConnectedBias = INDArrayWeight(Nd4j.zeros(Cifar10.NumberOfClasses))\n",
    "//         Model(cnnLayerParameter, fullConnectedWeight, fullConnectedBias)\n",
    "//     }\n",
    "    \n",
    "//     if (isExistVersion(version)) {\n",
    "//         loadWeightAndBias()\n",
    "//     } else {\n",
    "//         randomlyInitializeWeightAndBias()\n",
    "//     }\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// val currentVersion = 0\n",
    "// val model = initializeWeightAndBias(currentVersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// for (CnnLayerParameter(weight, bias) <- model.cnnLayerParameters) {\n",
    "//     hyperparameters.logger.info(s\"${weight.data.shape.toSeq} ${bias.data.shape.toSeq}\")\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// def dropOut(layer: INDArrayLayer, p: Double): INDArrayLayer = {\n",
    "//     import com.thoughtworks.raii.asynchronous._\n",
    "//     import scalaz.syntax.all._\n",
    "//     import com.thoughtworks.deeplearning.DeepLearning.Tape\n",
    "// //     import org.nd4j.linalg.api.ndarray.INDArray._\n",
    "// //     import org.nd4s.Implicits._\n",
    "//     val doTape: Do[Tape[INDArray, INDArray]] = layer.forward.flatMap { tape: Tape[INDArray, INDArray] =>\n",
    "//         (layer * (Nd4j.randn(tape.data.shape) gt p)).forward\n",
    "//     }\n",
    "//     INDArrayLayer(doTape)\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// def dropOut(layer: INDArrayLayer, p: Double): INDArrayLayer = INDArrayLayer(monadic[Do] {\n",
    "//     (layer * (Nd4j.randn(layer.forward.each.data.shape) > p)).forward.each\n",
    "// })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mKernelSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m7\u001b[39m\n",
       "\u001b[36mKernelWidth\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m7\u001b[39m\n",
       "\u001b[36mKernelHeight\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m7\u001b[39m\n",
       "\u001b[36mNumFilters\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m32\u001b[39m\n",
       "\u001b[36mHiddenDim\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m100\u001b[39m\n",
       "\u001b[36mWeightScale\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.01\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mPixelHeight\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mPixelWidth\u001b[39m\n",
       "\u001b[36mPadding\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3\u001b[39m\n",
       "\u001b[36mStride\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m1\u001b[39m\n",
       "\u001b[36mPoolSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m2\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val KernelSize: Int = 7\n",
    "val KernelWidth: Int = KernelSize\n",
    "val KernelHeight: Int = KernelSize\n",
    "\n",
    "val NumFilters: Int = 32\n",
    "val HiddenDim: Int = 100 //define hidden_layer->affineRuleOfCnnLayer\n",
    "val WeightScale: Double = 1e-2\n",
    "// def defaultPixelSize = Cifar10.Width\n",
    "def PixelHeight = Cifar10.Height\n",
    "def PixelWidth = Cifar10.Width\n",
    "val Padding: Int = (KernelSize - 1) / 2\n",
    "val Stride: Int = 1\n",
    "val PoolSize: Int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mAllWeightsAndBias\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object AllWeightsAndBias {\n",
    "    val cnnWeight: INDArrayWeight = INDArrayWeight(Nd4j.randn(Array(NumFilters, Cifar10.NumberOfChannels, KernelHeight, KernelWidth)) mul WeightScale)\n",
    "    val cnnBias: INDArrayWeight = INDArrayWeight(Nd4j.zeros(NumFilters))\n",
    "    val affineWeight: INDArrayWeight = INDArrayWeight(Nd4j.randn(Array(NumFilters * (PixelHeight / PoolSize) * (PixelWidth / PoolSize), HiddenDim)) mul WeightScale)\n",
    "    val affineBias: INDArrayWeight = INDArrayWeight(Nd4j.zeros(HiddenDim))\n",
    "    val affineLastWeight: INDArrayWeight = INDArrayWeight(Nd4j.randn(Array(HiddenDim, Cifar10.NumberOfClasses)) mul WeightScale)\n",
    "    val affineLastBias: INDArrayWeight = INDArrayWeight(Nd4j.zeros(Cifar10.NumberOfClasses))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mAllWeightsAndBias._\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import AllWeightsAndBias._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36maffine\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrelu\u001b[39m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def affine(input: INDArrayLayer, weight: INDArrayWeight, bias: INDArrayWeight): INDArrayLayer = {\n",
    "    input dot weight + bias\n",
    "}\n",
    "\n",
    "def relu(input: INDArrayLayer): INDArrayLayer = {\n",
    "    import hyperparameters.max\n",
    "    max(input, 0.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmyNeuralNetwork\u001b[39m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// def __init__(self, input_dim=(3, 32, 32), num_filters=32, filter_size=7,\n",
    "//                hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0,\n",
    "//                dtype=np.float32):\n",
    "\n",
    "//     C, H, W = input_dim\n",
    "//     F, HH, WW = num_filters, filter_size, filter_size\n",
    "//     self.params['W1'] = weight_scale * np.random.randn(F, C, HH, WW)\n",
    "//     self.params['W2'] = weight_scale * np.random.randn(F*H/2*W/2, hidden_dim)\n",
    "//     self.params['W3'] = weight_scale * np.random.randn(hidden_dim, num_classes)\n",
    "//     self.params['b1'] = np.zeros(F)\n",
    "//     self.params['b2'] = np.zeros(hidden_dim)\n",
    "//     self.params['b3'] = np.zeros(num_classes)\n",
    "\n",
    "// INDArrayWeight(Nd4j.randn(\n",
    "//                     Array(numberOfFilters, \n",
    "//                     inputDepth, \n",
    "//                     KernelWidth, \n",
    "//                     KernelHeight)\n",
    "//                 ) / math.sqrt(inputDepth * KernelWidth * KernelHeight / 2))\n",
    "\n",
    "def myNeuralNetwork(input: INDArray):  INDArrayLayer = {\n",
    "    import hyperparameters.max\n",
    "    import hyperparameters.maxPool\n",
    "    import hyperparameters.conv2d\n",
    "    \n",
    "    val cnnLayer = maxPool(relu(conv2d(input.reshape(input.shape()(0), Cifar10.NumberOfChannels, PixelHeight, PixelWidth), cnnWeight, cnnBias, (KernelHeight, KernelWidth), (Stride, Stride), (Padding, Padding))), (PoolSize, PoolSize))\n",
    "\n",
    "    val affineRuleOfCnnLayer = relu(affine(cnnLayer.reshape(input.shape()(0), NumFilters * (PixelHeight / PoolSize) * (PixelWidth / PoolSize)), affineWeight, affineBias))\n",
    "\n",
    "    val affineOfaffineRuleOfCnnLayer = affine(affineRuleOfCnnLayer.reshape(input.shape()(0), HiddenDim), affineLastWeight, affineLastBias)\n",
    "\n",
    "    val softmaxValue = softmax(affineOfaffineRuleOfCnnLayer)\n",
    "\n",
    "    softmaxValue\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// def myNeuralNetwork(input: INDArray):  INDArrayLayer = {\n",
    "//     import hyperparameters.max\n",
    "//     import hyperparameters.maxPool\n",
    "//     import hyperparameters.conv2d\n",
    "    \n",
    "//     val Model(cnnLayerParameters, fullConnectedWeight, fullConnectedBias) = model\n",
    "    \n",
    "//     def loop(i: Int): INDArrayLayer = {\n",
    "//         val CnnLayerParameter(weight, bias) = cnnLayerParameters(i)\n",
    "//         val cnnLayer = if (i == 0) {\n",
    "//             max(conv2d(input, weight, bias, (3, 3), (1, 1), (1, 1)), 0.0)\n",
    "//         } else {\n",
    "//             max(conv2d(loop(i - 1), weight, bias, (3, 3), (1, 1), (1, 1)), 0.0)\n",
    "//         }\n",
    "//         if (cnnLayers(i).hasPooling) {\n",
    "//             maxPool(cnnLayer, (2, 2))\n",
    "//         } else {\n",
    "//             cnnLayer\n",
    "//         }\n",
    "//     }\n",
    "    \n",
    "//     val layer6 = loop(5)\n",
    "    \n",
    "    \n",
    "//     // ??? Width ?= Height\n",
    "//     val layer6DropOut = dropOut(layer6, 0.5)\n",
    "//     val layer7 = layer6DropOut.reshape(input.shape.head, outputPixel) dot fullConnectedWeight + fullConnectedBias\n",
    "//     softmax(layer7)\n",
    "// }\n",
    "\n",
    "\n",
    "// def myNeuralNetwork(input: INDArray): INDArrayLayer = {\n",
    "//     import hyperparameters.max\n",
    "//     import hyperparameters.maxPool\n",
    "//     import hyperparameters.conv2d\n",
    "//     val layer1 = maxPool(max(conv2d(input.reshape(input.shape()(0), 3, 32, 32), weight1, bias1, (3, 3), (1, 1), (1, 1)), 0.0), (2, 2))\n",
    "//     val layer2 = maxPool(max(conv2d(layer1, weight2, bias2, (3, 3), (1, 1), (1, 1)), 0.0), (2, 2))\n",
    "//     val layer3 = maxPool(max(conv2d(layer2, weight3, bias3, (3, 3), (1, 1), (1, 1)), 0.0), (2, 2))\n",
    "//     val layer4 = maxPool(max(conv2d(layer3, weight4, bias4, (3, 3), (1, 1), (1, 1)), 0.0), (2, 2))\n",
    "//     val layer5 = maxPool(max(conv2d(layer4, weight5, bias5, (3, 3), (1, 1), (1, 1)), 0.0), (2, 2))\n",
    "\n",
    "//     val layer6 = layer5.reshape(input.shape()(0), 24) dot weight6 + bias6\n",
    "//     softmax(layer6)\n",
    "// }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LossFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about the prediction result of the neural network, we need to write the loss function `lossFunction`. We use [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy) to make comparison between this result and the actual result before return the score. Formula:\n",
    "![](https://zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhyperparameters.DoubleLayer\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hyperparameters.DoubleLayer\n",
    "\n",
    "def lossFunction(input: INDArray, expectOutput: INDArray): DoubleLayer = {\n",
    "\n",
    "    \n",
    "    val probabilities = myNeuralNetwork(input)\n",
    "    val result = -(hyperparameters.log(probabilities) * expectOutput).mean\n",
    "    \n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the images and corresponding label information for test data from CIFAR10 database and process them, we need [`import $file.ReadCIFAR10ToNDArray`](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/ReadCIFAR10ToNDArray.sc). This is a script file containing the read and processed CIFAR10 data, provided in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// import $url.{`https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/v1.0.0-doc/ipynbs/ReadCIFAR10ToNDArray.sc` => ReadCIFAR10ToNDArray}\n",
    "\n",
    "// val trainNDArray = ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/data_batch_1.bin\", 1000)\n",
    "\n",
    "// val testNDArray = ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing data to the softmax classifier, we first process label data with ([one hot encoding](https://en.wikipedia.org/wiki/One-hot)): transform INDArray of `NumberOfPixels Ã— 1` into INDArray of `NumberOfPixels Ã— NumberOfClasses`. The value of correct classification corresponding to each line is 1, and the values of other columns are 0. The reason for differentiating the training set and test set is to make it clear that whether the network is over trained which leads to [overfitting](https://en.wikipedia.org/wiki/Overfitting). While processing label data, we used [Utils](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/Utils.sc), which is also provided in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// val trainData = trainNDArray.head\n",
    "// val testData = testNDArray.head\n",
    "\n",
    "\n",
    "// val trainExpectResult = trainNDArray.tail.head\n",
    "// val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "// import $url.{`https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/v1.0.0-doc/ipynbs/Utils.sc` => Utils}\n",
    "\n",
    "// val vectorizedTrainExpectResult = Utils.makeVectorized(trainExpectResult, NumberOfClasses)\n",
    "// val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe the training process of the neural network, we need to output `loss`; while training the neural network, the `loss` shall be decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mTrainer\u001b[39m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// var lossSeq: IndexedSeq[Double] = IndexedSeq.empty\n",
    "\n",
    "// @monadic[Future]\n",
    "// val trainTask: Future[Unit] = {\n",
    "//   val lossStream = for (_ <- (1 to 2000).toStream) yield {\n",
    "//     val loss = lossFunction(trainData, vectorizedTrainExpectResult).train.each\n",
    "//     kernel.publish.markdown(s\"loss: $loss\")\n",
    "//     loss\n",
    "//   }\n",
    "//   lossSeq = IndexedSeq.concat(lossStream)\n",
    "// }\n",
    "\n",
    "\n",
    "class Trainer(batchSize: Int, numberOfEpoches: Int = 5) {\n",
    "    import scalaz.std.anyVal._\n",
    "    import scalaz.syntax.all._\n",
    "    @volatile\n",
    "    private var isShuttingDown: Boolean = false\n",
    "\n",
    "    private val lossBuffer = scala.collection.mutable.Buffer.empty[Double]\n",
    "        \n",
    "    def poltLoss(): Unit = Seq(Scatter(lossBuffer.indices, lossBuffer)).plot(title = \"loss by time\")\n",
    "    \n",
    "    def interrupt(): Unit = isShuttingDown = true\n",
    "\n",
    "    def startTrain(): Unit = {\n",
    "\n",
    "        @monadic[Future]\n",
    "        def trainTask: Future[Unit] = {\n",
    "            isShuttingDown = false\n",
    "            var epoch = 0\n",
    "            \n",
    "            while (epoch < numberOfEpoches && !isShuttingDown) {\n",
    "                val iterator = cifar10.epoch(batchSize).zipWithIndex\n",
    "                while (iterator.hasNext && !isShuttingDown) {\n",
    "                    val (Cifar10.Batch(labels, batch), i) = iterator.next()\n",
    "                    val loss = lossFunction(batch, labels).train.each\n",
    "                    lossBuffer += loss\n",
    "                    hyperparameters.logger.info(s\"epoch=$epoch iteration=$i batchSize=$batchSize loss=$loss\")\n",
    "                }\n",
    "                epoch += 1\n",
    "            }\n",
    "            \n",
    "            hyperparameters.logger.info(\"Done\")\n",
    "        }\n",
    "            trainTask.onComplete { tryUnit: scala.util.Try[Unit] => tryUnit.get }\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainBatchSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m50\u001b[39m\n",
       "\u001b[36mtrainer\u001b[39m: \u001b[32mTrainer\u001b[39m = $sess.cmd23Wrapper$Helper$Trainer@19eae072"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainBatchSize = 50\n",
    "\n",
    "val trainer = new Trainer(batchSize = trainBatchSize, numberOfEpoches = 100)\n",
    "trainer.startTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.interrupt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Serialization\n",
    "// writeWeightsAndBias(version = currentVersion, model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict  your Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the processed test data to verify the prediction result of the neural network and compute the accuracy. The accuracy shall be about 32%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Interrupted!",
      "  sun.misc.Unsafe.park(\u001b[32mNative Method\u001b[39m)",
      "  java.util.concurrent.locks.LockSupport.park(\u001b[32mLockSupport.java\u001b[39m:\u001b[32m175\u001b[39m)",
      "  java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(\u001b[32mAbstractQueuedSynchronizer.java\u001b[39m:\u001b[32m836\u001b[39m)",
      "  java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(\u001b[32mAbstractQueuedSynchronizer.java\u001b[39m:\u001b[32m997\u001b[39m)",
      "  java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(\u001b[32mAbstractQueuedSynchronizer.java\u001b[39m:\u001b[32m1304\u001b[39m)",
      "  scala.concurrent.impl.Promise$DefaultPromise.tryAwait(\u001b[32mPromise.scala\u001b[39m:\u001b[32m206\u001b[39m)",
      "  scala.concurrent.impl.Promise$DefaultPromise.ready(\u001b[32mPromise.scala\u001b[39m:\u001b[32m222\u001b[39m)",
      "  scala.concurrent.impl.Promise$DefaultPromise.result(\u001b[32mPromise.scala\u001b[39m:\u001b[32m227\u001b[39m)",
      "  scala.concurrent.Await$$anonfun$result$1.apply(\u001b[32mpackage.scala\u001b[39m:\u001b[32m190\u001b[39m)",
      "  scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(\u001b[32mBlockContext.scala\u001b[39m:\u001b[32m53\u001b[39m)",
      "  scala.concurrent.Await$.result(\u001b[32mpackage.scala\u001b[39m:\u001b[32m190\u001b[39m)",
      "  $sess.cmd26Wrapper$Helper.<init>(\u001b[32mcmd26.sc\u001b[39m:\u001b[32m36\u001b[39m)",
      "  $sess.cmd26Wrapper.<init>(\u001b[32mcmd26.sc\u001b[39m:\u001b[32m675\u001b[39m)",
      "  $sess.cmd26$.<init>(\u001b[32mcmd26.sc\u001b[39m:\u001b[32m429\u001b[39m)",
      "  $sess.cmd26$.<clinit>(\u001b[32mcmd26.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "// val predictResult = Await.result(myNeuralNetwork(testData).predict.toScalaFuture, Duration.Inf)\n",
    "\n",
    "// myNeuralNetwork\n",
    "\n",
    "def findMaxItemIndex(iNDArray: INDArray): INDArray = {\n",
    "    Nd4j.argMax(iNDArray, 1)\n",
    "}\n",
    "\n",
    "// def getAccuracy(score: INDArray, testExpectLabel: INDArray): Double = {\n",
    "//     val scoreIndex = findMaxItemIndex(score)\n",
    "//     val numberOfCorrectPrediction = (0 until scoreIndex.shape()(0)).count { row =>\n",
    "//         scoreIndex.getDouble(row, 0) == testExpectLabel.getDouble(row, 0)\n",
    "//     }\n",
    "//     (numberOfCorrectPrediction / score.shape()(0)) * 100\n",
    "// }\n",
    "\n",
    "def getAccuracy(score: INDArray, testExpectLabel: INDArray): Double = {\n",
    "    import org.nd4s.Implicits._\n",
    "    val scoreIndex = findMaxItemIndex(score)\n",
    "    if (testExpectLabel.shape().toSeq.last == 1) { //not vectorized\n",
    "      val acc = for (row <- 0 until scoreIndex.shape()(0)) yield {\n",
    "        if (scoreIndex.getDouble(row, 0) ==\n",
    "              testExpectLabel.getDouble(row, 0)) {\n",
    "          1.0\n",
    "        } else 0.0\n",
    "      }\n",
    "      (acc.sum / score.shape()(0)) * 100\n",
    "    } else if (testExpectLabel.shape().toSeq.last == 10) { //vectorized\n",
    "      val expectResultIndex = findMaxItemIndex(testExpectLabel)\n",
    "      val accINDArray = scoreIndex.eq(expectResultIndex)\n",
    "      (accINDArray.sumT / score.shape()(0)) * 100\n",
    "    } else\n",
    "      throw new IllegalArgumentException(\"Unacceptable testExpectLabel\")\n",
    "}\n",
    "\n",
    "val accuracyResultBuffer = scala.collection.mutable.Buffer.empty[Double]\n",
    "val iterator = cifar10.testBatches(trainBatchSize)\n",
    "while (iterator.hasNext) {\n",
    "    val Cifar10.Batch(testDatalabels, testDataBatch) = iterator.next()\n",
    "    val predictResult = Await.result(myNeuralNetwork(testDataBatch).predict.toScalaFuture, Duration.Inf)\n",
    "    val accuracyResult = getAccuracy(predictResult ,testDatalabels)\n",
    "    accuracyResultBuffer += accuracyResult\n",
    "}\n",
    "\n",
    "val accuracy = accuracyResultBuffer.sum / accuracyResultBuffer.length\n",
    "\n",
    "println(\"The accuracy is \" + accuracy + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.poltLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have learned the follows in this article:\n",
    "\n",
    "* Prepare and process CIFAR10 data\n",
    "* Write softmax classifier\n",
    "* Use the prediction image of the neural network written by softmax classifier to match with the probability of each category."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
